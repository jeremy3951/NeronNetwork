#include <assert.h>
#include <math.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

struct ann;
typedef double (*ann_act_func)(const struct ann *ctx, double a);

typedef struct ann {
    int inputs, hidden_layers, hidden, outputs;
    ann_act_func activation_hidden; /* used for hidden neurons */
    ann_act_func activation_output; /* used for output */
    int total_weights;              /* weights and size of weights buffer */
    int total_neurons; /* neurons + inputs and size of output buffer */
    double *weight;    /* All weights */
    double *output;    /* store input array and output of each neuron */
    double *delta;     /* total_neurons - inputs */
} ann_t;

#define LOOKUP_SIZE 4096

double lr = 0.1;

static inline double ann_act_hidden_indirect(const struct ann *ctx, double a) {
    return ctx->activation_hidden(ctx, a);
}

static inline double ann_act_output_indirect(const struct ann *ctx, double a) {
    return ctx->activation_output(ctx, a);
}



static const double sigmoid_dom_min = -15.0, sigmoid_dom_max = 15.0;
static double interval, lookup[LOOKUP_SIZE];

#define unlikely(x) __builtin_expect(!!(x), 0)
#define UNUSED __attribute__((unused))

static double ann_act_sigmoid(const ann_t *ctx UNUSED, double a) {
    if (a < -45.0)
        return 0;
    if (a > 45.0)
        return 1;
    return 1.0 / (1 + exp(-a));
}

static void ann_init_sigmoid_lookup(const ann_t *ctx) {
    const double f = (sigmoid_dom_max - sigmoid_dom_min) / LOOKUP_SIZE;

    interval = LOOKUP_SIZE / (sigmoid_dom_max - sigmoid_dom_min);
    for (int i = 0; i < LOOKUP_SIZE; ++i)
        lookup[i] = ann_act_sigmoid(ctx, sigmoid_dom_min + f * i);
}

static double ann_act_sigmoid_cached(const ann_t *ctx UNUSED, double a) {
    assert(!isnan(a));

    if (a < sigmoid_dom_min)
        return lookup[0];
    if (a >= sigmoid_dom_max)
        return lookup[LOOKUP_SIZE - 1];

    size_t j = (a - sigmoid_dom_min) * interval + 0.5;
    if (unlikely(j >= LOOKUP_SIZE))
        return lookup[LOOKUP_SIZE - 1];

    return lookup[j];
}

/* conventional generator (splitmix) developed by Steele et al. */
static uint64_t splitmix_x;
static inline uint64_t splitmix() {
    splitmix_x += 0x9E3779B97F4A7C15;
    uint64_t z = splitmix_x;
    z = (z ^ (z >> 30)) * 0xBF58476D1CE4E5B9;
    z = (z ^ (z >> 27)) * 0x94D049BB133111EB;
    return z ^ (z >> 31);
}

static void splitmix_init() {
    /* Hopefully, ASLR makes our function address random */
    uintptr_t x = (uintptr_t)((void *) &splitmix_init);
    struct timespec time;
    clock_gettime(CLOCK_MONOTONIC, &time);
    x ^= (uintptr_t) time.tv_sec;
    x ^= (uintptr_t) time.tv_nsec;
    splitmix_x = x;

    /* do a few randomization steps */
    uintptr_t max = ((x ^ (x >> 17)) & 0x0F) + 1;
    for (uintptr_t i = 0; i < max; i++)
        splitmix();
}

/* uniform random numbers between 0 and 1 */
#define ANN_RAND() (((double) splitmix()) / UINT64_MAX)

/* Set weights randomly */
void ann_randomize(ann_t *ctx) {
    for (int i = 0; i < ctx->total_weights; ++i)
        ctx->weight[i] = ANN_RAND() -0.5; /* weights from -0.5 to 0.5 */
}

/* Create and return a new network context */
ann_t *ann_init(int inputs, int hidden_layers, int hidden, int outputs) {
    if (hidden_layers < 0)
        return 0;
    if (inputs < 1)
        return 0;
    if (outputs < 1)
        return 0;
    if (hidden_layers > 0 && hidden < 1)
        return 0;

    const int hidden_weights =
        hidden_layers ? (inputs + 1) * hidden +
                            (hidden_layers - 1) * (hidden + 1) * hidden
                      : 0;
    const int output_weights =
        (hidden_layers ? (hidden+1) : (inputs + 1)) * outputs;
    const int total_weights = (hidden_weights + output_weights);

    const int total_neurons = (inputs + hidden * hidden_layers + outputs);

    /* Allocate extra size for weights, outputs, and deltas. */
    const int size =
        sizeof(ann_t) + sizeof(double) * (total_weights + total_neurons +
                                          (total_neurons - inputs));



    ann_t *ret = malloc(size);
    assert(ret);

    ret->inputs = inputs;
    ret->hidden_layers = hidden_layers;
    ret->hidden = hidden;
    ret->outputs = outputs;

    ret->total_weights = total_weights;
    ret->total_neurons = total_neurons;

    ret->weight =  ((char *) ret + sizeof(ann_t));
    ret->output = ret->weight + ret->total_weights;
    ret->delta = ret->output + ret->total_neurons;/* neuron output ¦s³o*/

    ann_randomize(ret);

    ret->activation_hidden = ann_act_sigmoid_cached;
    ret->activation_output = ann_act_sigmoid_cached;

    ann_init_sigmoid_lookup(ret);
    return ret;
}

/* Return a new copy of network context */
ann_t *ann_copy(ann_t const *ctx) {
    const int size = sizeof(ann_t) +
                     sizeof(double) * (ctx->total_weights + ctx->total_neurons +
                                       (ctx->total_neurons - ctx->inputs));
    ann_t *ret = malloc(size);
    assert(ret);

    memcpy(ret, ctx, size);

    ret->weight = (double *) ((char *) ret + sizeof(ann_t));
    ret->output = ret->weight + ret->total_weights;
    ret->delta = ret->output + ret->total_neurons;

    return ret;
}

/* Free the memory used by a network context */
void ann_free(ann_t *ctx) { free(ctx); }



/* Run the feedforward algorithm to calculate the output of the network. */

double const *ann_run(ann_t const *ctx, double const *inputs) {
    double const *w = ctx->weight;
    double *o = ctx->output + ctx->inputs;
    double const *i = ctx->output;



    /* Copy the inputs to the scratch area, where we also store each neuron's
     * output, for consistency. This way the first layer isn't a special case.
     */
    memcpy(ctx->output, inputs, sizeof(double) * ctx->inputs);

    if (!ctx->hidden_layers) {
        double *ret = o;
        for (int j = 0; j < ctx->outputs; ++j) {
            double sum = *w++ * (-1.0);
            for (int k = 0; k < ctx->inputs; ++k)
                sum += *w++ * i[k];
            *o++ = ann_act_output_indirect(ctx, sum);
        }
        return ret;
    }

    /* Figure input layer */
    for (int j = 0; j < ctx->hidden; ++j) {
        double sum = *w++ * (-1.0);

        for (int k = 0; k < ctx->inputs; ++k)
            sum += *w++ * i[k] ;
        *o++ = ann_act_hidden_indirect(ctx, sum) ;
    }
    i += ctx->inputs;

    /* Figure hidden layers, if any. */
    for (int h = 1; h < ctx->hidden_layers; ++h) {
        for (int j = 0; j < ctx->hidden; ++j) {
            double sum = *w++ * (-1.0);

            for (int k = 0; k < ctx->hidden; ++k)
                sum += *w++ * i[k] ;
            *o++ = ann_act_hidden_indirect(ctx, sum) ;
        }
        i += ctx->hidden;
    }
    double const *ret = o;

    /* Figure output layer. */
    for (int j = 0; j < ctx->outputs; ++j) {
        double sum = *w++ * (-1.0);

        for (int k = 0; k < ctx->hidden; ++k)
            sum += *w++ * i[k] ;
        *o++ = ann_act_output_indirect(ctx, sum) ;
    }

    assert(w - ctx->weight == ctx->total_weights);
    assert(o - ctx->output == ctx->total_neurons);

    return ret;
}

void backward_in(ann_t *ctx ,int bias, double *in , double *df , double *gradient){

    int hidden = ctx->hidden;
    double dc[hidden];
    double sig[hidden];
    bias = bias * ctx->inputs;

    int w_idx2 = 0;
    int w_idx1 =(ctx->inputs +1)*hidden;//越過 bias

    for(int j = 0; j< ctx->hidden ; j++){
        /* 算 neuron 的偏微分 */
        (w_idx1++);
        for(int k = 0; k< ctx->hidden;k++){
            if(k==0){dc[j] = ctx->weight[(w_idx1++)+k] * df[k];}
                else{dc[j] += ctx->weight[(w_idx1++)+k] * df[k];}
        }
        /* 算 neuron 的 sigmoid 偏微分 */
        sig[j] = ctx->output[j+ctx->inputs] * (1- ctx->output[j+ctx->inputs]) * dc[j] ;

        /* 算 gradient */
        gradient[w_idx2++] = 1 * sig[j];

        //printf("test%d %lf\n",j,gradient[w_idx2]);
        for(int i = 0; i< ctx->inputs ; i++){
            gradient[w_idx2++] = *(in+bias+j) * sig[j];
        }
    }

}

void backward_hidden( int layer ,int bias,ann_t *ctx , double *in , double *gradient , double *df){
    //index =  10
    int hidden = ctx->hidden;
    double dc[hidden];
    double sig[hidden];
    int last_neuron = (layer==1?ctx->outputs:ctx->hidden);
    int layer_sub = ctx->hidden_layers - layer;


    int w_idx2 = (ctx->inputs+1) * hidden + (layer_sub-1)*(hidden+1)*hidden ;
    int w_idx1 = w_idx2 + (hidden+1)*hidden;//越過 bias

    for(int j = 0; j< ctx->hidden ; j++){
        /* 算 neuron 的偏微分 */
        w_idx1++;
        for(int k = 0; k< last_neuron;k++){
            if(k==0){dc[j] = ctx->weight[(w_idx1++)+k] * df[k];}
                else{dc[j] += ctx->weight[(w_idx1++)+k] * df[k];}
        }
        /* 算 neuron 的 sigmoid 偏微分 */
        sig[j] = ctx->output[layer_sub*hidden+j+ctx->inputs] * (1- ctx->output[layer_sub*hidden+j+ctx->inputs]) * dc[j] ;

        /* 算 gradient */

        gradient[w_idx2++] = 1 * sig[j];

        for(int i = 0; i< ctx->hidden ; i++){
            gradient[w_idx2++] = ctx->output[(layer_sub-1)*hidden+i+ctx->inputs] * sig[j];
        }
    }

    if(layer==ctx->hidden_layers){
        backward_in(ctx ,bias, in , sig , gradient);
    }else{
        backward_hidden(layer+1 ,bias, ctx , in , gradient ,sig );
    }


}

void backward_out( ann_t *ctx , int bias,double ans , double *in , double *gradient){

    /*
    double df = 0.0;

    df = res[3*i+2] * ( 1- res[3*i+2] ) * 2*ans[i];

    gradient[10] = 1 * df;

    gradient[9]  = res[3*i+1] * df ;

    gradient[8]  = res[3*i] * df ;

    backward( w , in , res , gradient , 3*i ,df);
    */

    int n_idx = ctx->hidden_layers * ctx->hidden +ctx->inputs;
    double df[ctx->outputs];
    int ind = n_idx - ctx->outputs ;
    int w_idx = ctx->total_weights - (ctx->hidden+1) * ctx->outputs ;

    for(int i = 0; i< ctx->outputs ; i++){

        double t = ans; //假設都一樣
        double y = ctx->output[n_idx+i];

        //printf("test %lf\n",y);


        df[i] = y * (1-y) * 2 * ans;

        gradient[ w_idx++] = 1 * df[i] ;

        for(int j = 0; j< ctx->hidden ; j++){

            gradient[ w_idx++] = ctx->output[ (n_idx - ctx->hidden) + j ] * df[i] ;
            //printf("test%d %lf\n",j,gradient[ w_idx]);
        }


    }

    if(ctx->hidden_layers==1){

        backward_in(ctx ,bias, in , df , gradient);
    }else{
        backward_hidden( 1 ,bias, ctx , in , gradient ,df);
    }



}


#define LOOP_MAX 1024

int main(int argc, char *argv[]) {
    //splitmix_init();


    /* Input and expected out data for the 2-input Multiplexer */
    const double input[8][3] = {
        {0, 0, 0}, {0, 0, 1}, {0, 1, 0}, {0, 1, 1},
        {1, 0, 0}, {1, 0, 1}, {1, 1, 0}, {1, 1, 1},
    };
    double *ptr_to_in = input ;

    const double output[8] = {
        0, 0, 1, 1, 0, 1, 0, 1,
    };
    double *ans = output ;

    /* New network with 3 inputs, 1 hidden layer of 2 neurons,
     * and 1 output.
     */
    ann_t *ctx = ann_init(3, 1, 2, 1);


    double *gradient = malloc(sizeof(double)*(ctx->total_weights) ) ;


    double err, last_err = LOOP_MAX;
    err = 20.0;
    int count = 0;
    do {
        for(int i =0 ;i< ctx->total_weights ;i++){
            gradient[i] = 0.0;
        }

        ++count;
        if (count % LOOP_MAX == 0) {
            ann_randomize(ctx);
            last_err = LOOP_MAX;
        }

        ann_t *save = ann_copy(ctx);

        /* Take a random guess at the ANN weights. */
        /*for (int i = 0; i < ctx->total_weights; ++i)
            ctx->weight[i] += ANN_RAND() -0.5 ;*/

        err --;
//        printf("%lf \n" , *ann_run(ctx, input[1]));
        for (int k = 0; k < (1 << 3); k++){
            //err += pow(*ann_run(ctx, input[k]) - output[k], 2.0);
            double tt = *ann_run(ctx, input[k]) - output[k];
            //printf("out%d is: %lf\n",k,*ann_run(ctx, input[k]));

            backward_out(ctx ,k, ans[k] , ptr_to_in , gradient);


/*---------------------------------update--------------------------*/
            for(int i =0; i< ctx->total_weights;i++){

                //printf("g%d is: %lf\n",i,gradient[i]);
                ctx->weight[i] -= lr * gradient[i];
            }


            //printf("out: %lf  - ans: %lf \n" , *ann_run(ctx, input[k]) , output[k] );

            //backward2( k , ptr_to_in , ctx->weight , ctx->output+3 , gradient , tt , ctx );

            /*printf("o1 %lf \n" , ctx->output[3]);
            printf("o2 %lf \n" , ctx->output[4]);
            printf("o3 %lf \n" , ctx->output[5]);*/

        }
        /* Keep these weights if they're an improvement. */
        /*
        if (err < last_err) {
            ann_free(save);
            last_err = err;
        } else {
            ann_free(ctx);
            ctx = save;
        }*/
    } while (err > 0);
    printf("Finished in %d loops.\n", count);

    /* Run the network and see what it predicts. */
    for (int k = 0; k < (1 << 3); k++)
        printf("[%1.f, %1.f, %1.f] = %lf.\n", input[k][0], input[k][1],
               input[k][2], *ann_run(ctx, input[k]));

    ann_free(ctx);
    return 0;
}
